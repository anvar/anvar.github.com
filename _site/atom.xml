<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Anvar Karimson</title>
 <link href="http://karimson.com/atom.xml" rel="self"/>
 <link href="http://karimson.com/"/>
 <updated>2013-08-28T11:58:44+01:00</updated>
 <id>http://karimson.com</id>
 <author>
   <name>Anvar Karimson</name>
 </author>

 
 <entry>
   <title>Introduction to CUDA</title>
   <link href="http://karimson.com/posts/introduction-to-cuda"/>
   <updated>2013-08-26T00:00:00+01:00</updated>
   <id>http://karimson.com/posts/introduction-to-cuda</id>
   <content type="html">&lt;p&gt;GPUs; once strongly associated with gaming are now becoming common place in industry where frameworks like CUDA and OpenCL are exposing their power for general applications with serious need for computation. The key to their power is their parallelism. Where a standard CPU has between 4-8 cores, a GPU like the Nvidia GTX Titan readily comes with 2688 cores. The key, therefore, to successfully unlock their power is two-fold. The first part is to have an algorithm that can scale to the required level of parallelism. The second part, which may be less obvious, is to maximize GPU occupancy by carefully partitioning the computation to work with the hardware layout. In order to do that we must first look at what a GPU looks like at logical hardware level.&lt;/p&gt;

&lt;p&gt;The newest NVIDIA compute architecture is Kepler (compute capability 3.x), but here I will focus on a Fermi-based compute architecture (compute capability 2.x) as it is the one I have most familiarity with, and the one that Amazon EC2 uses in their Cluster GPU instance types.&lt;/p&gt;

&lt;p&gt;A Fermi-based GPU is based upon 16 Streaming Multiprocessors (SM) positioned around a common L2 cache, with each SM containing 32 cores, registers, and a small chunk of shared memory. Surrounding the SMs are six 64-bit memory partitions, for a 384-bit memory interface, supporting up to a total of 6GB of GDDR5 DRAM memory. The GigaScheduler provides a GPU-level scheduler distributing thread blocks to SMs internal schedulers, while the Host Interface connects through PCI-Express to the host system.&lt;/p&gt;

&lt;p&gt;Because an SM contains 32 cores it can, as such, only execute a maximum of 32 threads at any given time, which in CUDA-speak is called a warp. Every thread in a warp is executed in SIMD lockstep fashion, executing the same instruction but using its private registers to perform the requested operation. To get the most performance out of a GPU it is therefore important to understand warps, and how to write warp-friendly code. Warps are the primary way a GPU can hide latency, so if an operation will take a long time to perform - such as fetching from global memory - the warp scheduler will park the warp and schedule a different one. Once the memory access returns the warp will be rescheduled. By optimizing the memory access within warps the memory access can be coalesced with one call fetching data for several threads within the warp, and thus greatly reduce the overall cost of memory latency. Additionally, as the instructions are executed in lockstep, it is important to try to avoid conditional code which forces the threads within a warp to execute different branches as that can also have a significant impact on the time it takes to complete a warp.&lt;/p&gt;

&lt;h2 id='programming_model'&gt;Programming model&lt;/h2&gt;

&lt;p&gt;A program designed to run on a GPU is called a kernel, and in CUDA the level of parallelism for a kernel is defined by the grid size and the block size. The grid size defines the number of blocks and the shape of the cube that the blocks are distributed within. Ever since compute capability 2.0 the grid size can be specified in three dimensions, whilst in earlier versions being restricted to only two. The block size follows a similar model but has always had the ability to be specified in three dimensions.&lt;/p&gt;
&lt;img src='http://localhost:4000/assets/img/cuda-grid.png' class='center caption' width='305' height='290' /&gt;&lt;div class='caption'&gt;Hello, Caption!&lt;/div&gt;
&lt;h2 id='how_to_choose_size'&gt;How to choose size&lt;/h2&gt;

&lt;p&gt;In general, outside of the maximum allowed dimensions for blocks and grids, you want to size your blocks/grid to match your data and simultaneously maximise occupancy, that is, how many threads are active at one time. The major factors influencing occupancy are shared memory usage, register usage, and thread block size. Another factor to consider is that threads get scheduled in warps, so a block size should always be a multiple of 32, otherwise at least one warp will be scheduled that is not making use of all the cores in the SM. Picking the right dimensions is somewhat of a black art, as it can be GPU/kernel/data shape/algorithm dependent. Sometimes, for example, it makes sense to perform a bit more work in an individual thread to minimize the number of blocks that need to be scheduled. Therefore it is always good to experiment with various sizes to see what the impact is on your specific kernel.&lt;/p&gt;

&lt;p&gt;When a thread is executing a kernel there are a few variables that CUDA exposes that can help with identifying which thread it is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gridDim.{x,y,z} - The dimensions of the grid&lt;/li&gt;

&lt;li&gt;blockDim.{x,y,z} - The dimensions of the block&lt;/li&gt;

&lt;li&gt;blockIdx.{x,y,z} - The index of the current block within the grid&lt;/li&gt;

&lt;li&gt;threadIdx.{x,y,z} - The index of the current thread with the block&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id='cuda_hello_world'&gt;CUDA Hello World&lt;/h2&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='cuda'&gt;&lt;span class='kr'&gt;__global__&lt;/span&gt; 
&lt;span class='kt'&gt;void&lt;/span&gt; &lt;span class='nf'&gt;helloKernel&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;float&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='n'&gt;g_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='n'&gt;g_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;unsigned&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='c1'&gt;// put this in a comment in the code instead&lt;/span&gt;
  &lt;span class='kt'&gt;unsigned&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;global_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nb'&gt;blockIdx&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='nb'&gt;blockDim&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt; &lt;span class='nb'&gt;threadIdx&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;

  &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt; &lt;span class='o'&gt;&amp;lt;&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;g_out&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;g_in&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;g_in&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt;&lt;span class='p'&gt;];&lt;/span&gt;
  &lt;span class='p'&gt;}&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;

&lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='nf'&gt;main&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;argc&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;char&lt;/span&gt; &lt;span class='o'&gt;**&lt;/span&gt;&lt;span class='n'&gt;argv&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;dim3&lt;/span&gt; &lt;span class='n'&gt;blockSize&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;128&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt;
  &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;dim3&lt;/span&gt; &lt;span class='n'&gt;gridSize&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;256&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt;

  &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;h_in&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;new&lt;/span&gt; &lt;span class='kt'&gt;float&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;];&lt;/span&gt;

  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMalloc&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;&amp;amp;&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;2&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;
  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMemcpy&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='n'&gt;h_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
                             &lt;span class='n'&gt;cudaMemcpyHostToDevice&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;

  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;h_out&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;new&lt;/span&gt; &lt;span class='kt'&gt;float&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;];&lt;/span&gt;
  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMalloc&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;&amp;amp;&lt;/span&gt;&lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;

  &lt;span class='n'&gt;helloKernel&lt;/span&gt;&lt;span class='o'&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class='n'&gt;gridSize&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;blockSize&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt;

  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMemcpy&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;h_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
                             &lt;span class='n'&gt;cudaMemcpyDeviceToHost&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
 </entry>
 
 <entry>
   <title>Exploration-Friendly Design</title>
   <link href="http://karimson.com/posts/template"/>
   <updated>2013-01-01T00:00:00+00:00</updated>
   <id>http://karimson.com/posts/template</id>
   <content type="html">&lt;p&gt;I have been thinking a lot about UIs recently, and how some of them have a natural feel of exploration to them.&lt;/p&gt;

&lt;h2 id='siri'&gt;Siri&lt;/h2&gt;

&lt;p&gt;Take Siri as prime example. There is no manual, no instructions. Just a few phrases to get you started. Yet there are pages and pages out there dedicated solely to show off the functionality that people have been able to get out of Siri.&lt;/p&gt;
&lt;img src='http://localhost:4000/assets/img/cuda-grid.png' class='center' width='305' height='290' /&gt;
&lt;p&gt;I think the approach Apple has taken here is a very interesting one. Instead of announcing everything Siri is capable of, which would have been a safe choice, they settled for just enough to get you started.&lt;/p&gt;
&lt;img src='http://localhost:4000/assets/img/cuda-grid.png' class='center caption' width='305' height='290' /&gt;&lt;div class='caption'&gt;Hello, Caption!&lt;/div&gt;
&lt;p&gt;The result, I think, is a deeper attachment. Because no one handed you the full set of commands, you had to look for them or play around until you stumbled upon something cool. The key here is that they gave enough to get you started, otherwise you might not have been inclined to start playing with it in the first place.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='cuda'&gt;&lt;span class='kr'&gt;__global__&lt;/span&gt; 
&lt;span class='kt'&gt;void&lt;/span&gt; &lt;span class='nf'&gt;helloKernel&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;float&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='n'&gt;g_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='n'&gt;g_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;unsigned&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
&lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='kt'&gt;unsigned&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;global_id&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nb'&gt;blockIdx&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='nb'&gt;blockDim&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;+&lt;/span&gt; &lt;span class='nb'&gt;threadIdx&lt;/span&gt;&lt;span class='p'&gt;.&lt;/span&gt;&lt;span class='n'&gt;x&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;

  &lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt; &lt;span class='o'&gt;&amp;lt;&lt;/span&gt; &lt;span class='n'&gt;n&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
    &lt;span class='n'&gt;g_out&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;g_in&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;g_in&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;global_id&lt;/span&gt;&lt;span class='p'&gt;];&lt;/span&gt;
  &lt;span class='p'&gt;}&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;

&lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='nf'&gt;main&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;argc&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='kt'&gt;char&lt;/span&gt; &lt;span class='o'&gt;**&lt;/span&gt;&lt;span class='n'&gt;argv&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='p'&gt;{&lt;/span&gt;
  &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;dim3&lt;/span&gt; &lt;span class='n'&gt;blockSize&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;128&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt;
  &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;dim3&lt;/span&gt; &lt;span class='n'&gt;gridSize&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='mi'&gt;256&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt;

  &lt;span class='k'&gt;const&lt;/span&gt; &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='mi'&gt;10&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;h_in&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;new&lt;/span&gt; &lt;span class='kt'&gt;float&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;];&lt;/span&gt;

  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMalloc&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;&amp;amp;&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='mi'&gt;2&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;
  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMemcpy&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='n'&gt;h_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
                             &lt;span class='n'&gt;cudaMemcpyHostToDevice&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;

  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;h_out&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='n'&gt;new&lt;/span&gt; &lt;span class='kt'&gt;float&lt;/span&gt;&lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;];&lt;/span&gt;
  &lt;span class='kt'&gt;int&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt;&lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;;&lt;/span&gt;
  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMalloc&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='o'&gt;&amp;amp;&lt;/span&gt;&lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;

  &lt;span class='n'&gt;helloKernel&lt;/span&gt;&lt;span class='o'&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class='n'&gt;gridSize&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;blockSize&lt;/span&gt;&lt;span class='o'&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;d_in&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;);&lt;/span&gt;

  &lt;span class='n'&gt;checkCudaErrors&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;cudaMemcpy&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='n'&gt;h_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='n'&gt;d_out&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt;
                             &lt;span class='k'&gt;sizeof&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kt'&gt;int&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt; &lt;span class='o'&gt;*&lt;/span&gt; &lt;span class='n'&gt;number_of_elements&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; 
                             &lt;span class='n'&gt;cudaMemcpyDeviceToHost&lt;/span&gt;&lt;span class='p'&gt;));&lt;/span&gt;
&lt;span class='p'&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id='finance_uis_and_palantir'&gt;Finance UIs and Palantir&lt;/h2&gt;

&lt;p&gt;The realm I work in happens to be finance, and the UIs we have are not primarily tailored to exploration. They are mostly for efficiency, and sometimes, for the feel of efficiency. By chance, though, I stumbled upon Palantir and their financial software, and felt that exploration sense tingling again. Concept-wise what they provide is very simple. They give you a funnel, where you can apply filters to the data in order to constrain it and spit back just the right subset. They also give you a text box, where you can type queries. And as you type your queries, they show an auto-complete menu, to help you out a bit. That is all you need, really! And I find it works extremely well. It makes me want to play with the data. Explore it. Pull it apart, and put it together differently.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='clojure'&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='kd'&gt;defn &lt;/span&gt;&lt;span class='nv'&gt;foo&lt;/span&gt;
  &lt;span class='p'&gt;[&lt;/span&gt;&lt;span class='nv'&gt;bar&lt;/span&gt;&lt;span class='p'&gt;]&lt;/span&gt;
  &lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nf'&gt;baz&lt;/span&gt; &lt;span class='nv'&gt;bar&lt;/span&gt;&lt;span class='p'&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id='explorationfriendly_design'&gt;Exploration-friendly design&lt;/h2&gt;

&lt;p&gt;I call it exploration-friendly design. And it works really, really well. All of a sudden I can make comparisons between equities as quickly as I can think of them. How does the revenue of MSFT correlate to the employee count of GOOG? How does LON:RYAs revenue follow the number of hours that their planes spent on the ground. All of these questions I never knew I need an answer for are now within my reach and it feels nothing less than magical. Yet no one else seems to be doing this, not even close. The standard seems to be to make me click through menus and make simple static comparisons for correlations I already know about. There is no exploration. At least not for me. So I form no bond with the software, nothing that keeps me lingering around. I go there just to do my business and and soon as it is done, I leave for something more interesting.&lt;/p&gt;</content>
 </entry>
 

</feed>
